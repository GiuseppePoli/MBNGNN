{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V_mqdC8yY2C",
        "outputId": "76f305d4-de33-4f94-d484-e6d9c161f613"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import scipy.io\n",
        "import h5py\n",
        "import sklearn\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import ttest_ind, mannwhitneyu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data, Dataset, InMemoryDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "import os\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch import Tensor\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import random\n",
        "from itertools import product\n",
        "from sklearn.model_selection import train_test_split, KFold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek88DZVppIJU"
      },
      "source": [
        "# **Age Distribution Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avL8N9P93Pdp"
      },
      "outputs": [],
      "source": [
        "data_raw = pd.read_csv(r'C:\\Users\\polig\\Desktop\\TESI_DRIVE\\TESI\\data_raw_nmm_new.csv')\n",
        "strat_covars = pd.read_csv(r'C:\\Users\\polig\\Desktop\\TESI_DRIVE\\TESI\\MatchedData01.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jUyDAmt5etf",
        "outputId": "d5473e0f-8baf-49f1-fdba-23abe24b58ab"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "age_diagnosis_0 = strat_covars[strat_covars['Dx'] == 0]['Age']\n",
        "age_diagnosis_1 = strat_covars[strat_covars['Dx'] == 1]['Age']\n",
        "\n",
        "u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "\n",
        "print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_H_oXRSpb4Z",
        "outputId": "fafba66d-4d6a-425e-e8c6-862f7d782b5f"
      },
      "outputs": [],
      "source": [
        "sns.kdeplot(data=strat_covars, x='Age', hue='Dx', fill=True)\n",
        "plt.title('Age distribution by diagnosis')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_778d9Ix9NMU",
        "outputId": "705301db-e1fd-4b2c-9c99-a474ce27bc5b"
      },
      "outputs": [],
      "source": [
        "for batch in strat_covars['batch'].unique():\n",
        "\n",
        "    batch_data = strat_covars[strat_covars['batch'] == batch]\n",
        "\n",
        "    age_diagnosis_0 = batch_data[batch_data['Dx'] == 0]['Age']\n",
        "    age_diagnosis_1 = batch_data[batch_data['Dx'] == 1]['Age']\n",
        "\n",
        "    u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "\n",
        "    print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6y-_J2sNrlE"
      },
      "source": [
        "### Batch 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aljSLkD75QvG",
        "outputId": "f8f16845-324e-4adb-cd90-2fa5dc350562"
      },
      "outputs": [],
      "source": [
        "# Extract the training set covariates\n",
        "train_covars = strat_covars[strat_covars['batch']!=1]\n",
        "\n",
        "# Filter by diagnosis\n",
        "age_diagnosis_0 = train_covars[train_covars['Dx'] == 0]['Age']\n",
        "age_diagnosis_1 = train_covars[train_covars['Dx'] == 1]['Age']\n",
        "\n",
        "# Create the plot\n",
        "sns.kdeplot(age_diagnosis_0, label='Healthy (Dx=0)', fill=True)\n",
        "sns.kdeplot(age_diagnosis_1, label='BD (Dx=1)', fill=True)\n",
        "plt.title('Age Distribution in Training Set')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "t_stat, p_value = ttest_ind(age_diagnosis_0, age_diagnosis_1, equal_var=False)\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axfNKGivUfaC",
        "outputId": "377eea32-a81f-4438-972b-13418dba779d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define drop fractions\n",
        "drop_fraction_healthy = 0.2\n",
        "drop_fraction_bd = 0.2\n",
        "\n",
        "# Identify subjects to drop\n",
        "healthy_to_drop = train_covars[\n",
        "    (train_covars['Dx'] == 0) &\n",
        "    (train_covars['Age'] >= 15) &\n",
        "    (train_covars['Age'] <= 30)\n",
        "].sample(frac=drop_fraction_healthy, random_state=42)\n",
        "\n",
        "bd_to_drop = train_covars[\n",
        "    (train_covars['Dx'] == 1) &\n",
        "    (train_covars['Age'] >= 40) &\n",
        "    (train_covars['Age'] <= 60)\n",
        "].sample(frac=drop_fraction_bd, random_state=42)\n",
        "\n",
        "\n",
        "indices_to_remove_1 = set(healthy_to_drop.index).union(set(bd_to_drop.index))\n",
        "\n",
        "train_covars_new = train_covars.drop(indices_to_remove_1)\n",
        "\n",
        "\n",
        "print(f\"Removed {len(indices_to_remove_1)} subjects ({len(healthy_to_drop)} healthy, {len(bd_to_drop)} bd)\")\n",
        "print(f\"New training set size: {len(train_covars_new)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc_RHvBKUkPW",
        "outputId": "a4b5952d-aa9a-4a43-a2f0-f3afcc565095"
      },
      "outputs": [],
      "source": [
        "age_diagnosis_0 = train_covars_new[train_covars_new['Dx'] == 0]['Age']\n",
        "age_diagnosis_1 = train_covars_new[train_covars_new['Dx'] == 1]['Age']\n",
        "\n",
        "\n",
        "# Create the plot\n",
        "sns.kdeplot(age_diagnosis_0, label='Healthy (Dx=0)', fill=True)\n",
        "sns.kdeplot(age_diagnosis_1, label='BD (Dx=1)', fill=True)\n",
        "plt.title('Age Distribution in Training Set')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "t_stat, p_value = ttest_ind(age_diagnosis_0, age_diagnosis_1, equal_var=False)\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14KKJo13Pp7Y"
      },
      "source": [
        "### Batch 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0sMaJs2Pp7Z",
        "outputId": "7c4a9a18-23e4-40e5-9196-baa07d6d14b5"
      },
      "outputs": [],
      "source": [
        "# Extract the training set covariates\n",
        "train_covars = strat_covars[strat_covars['batch']!=3]\n",
        "\n",
        "# Filter by diagnosis\n",
        "age_diagnosis_0 = train_covars[train_covars['Dx'] == 0]['Age']\n",
        "age_diagnosis_1 = train_covars[train_covars['Dx'] == 1]['Age']\n",
        "\n",
        "# Create the plot\n",
        "sns.kdeplot(age_diagnosis_0, label='Healthy (Dx=0)', fill=True)\n",
        "sns.kdeplot(age_diagnosis_1, label='BD (Dx=1)', fill=True)\n",
        "plt.title('Age Distribution in Training Set')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "t_stat, p_value = ttest_ind(age_diagnosis_0, age_diagnosis_1, equal_var=False)\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAsFq_aiPp7a",
        "outputId": "89baea7a-736f-4aa1-a8d4-9e0b2113d952"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define drop fractions\n",
        "drop_fraction_healthy = 0.2\n",
        "drop_fraction_bd = 0.2\n",
        "\n",
        "# Identify subjects to drop\n",
        "healthy_to_drop = train_covars[\n",
        "    (train_covars['Dx'] == 0) &\n",
        "    (train_covars['Age'] >= 15) &\n",
        "    (train_covars['Age'] <= 30)\n",
        "].sample(frac=drop_fraction_healthy, random_state=42)\n",
        "\n",
        "bd_to_drop = train_covars[\n",
        "    (train_covars['Dx'] == 1) &\n",
        "    (train_covars['Age'] >= 40) &\n",
        "    (train_covars['Age'] <= 60)\n",
        "].sample(frac=drop_fraction_bd, random_state=42)\n",
        "\n",
        "\n",
        "indices_to_remove_3 = set(healthy_to_drop.index).union(set(bd_to_drop.index))\n",
        "\n",
        "train_covars_new = train_covars.drop(indices_to_remove_3)\n",
        "\n",
        "\n",
        "print(f\"Removed {len(indices_to_remove_3)} subjects ({len(healthy_to_drop)} healthy, {len(bd_to_drop)} bd)\")\n",
        "print(f\"New training set size: {len(train_covars_new)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgNCAJEcPp7a",
        "outputId": "30ad67d7-6525-4fce-90b4-6f59efe65d7c"
      },
      "outputs": [],
      "source": [
        "age_diagnosis_0 = train_covars_new[train_covars_new['Dx'] == 0]['Age']\n",
        "age_diagnosis_1 = train_covars_new[train_covars_new['Dx'] == 1]['Age']\n",
        "\n",
        "\n",
        "# Create the plot\n",
        "sns.kdeplot(age_diagnosis_0, label='Healthy (Dx=0)', fill=True)\n",
        "sns.kdeplot(age_diagnosis_1, label='BD (Dx=1)', fill=True)\n",
        "plt.title('Age Distribution in Training Set')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "t_stat, p_value = ttest_ind(age_diagnosis_0, age_diagnosis_1, equal_var=False)\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHkqSrIcRB_A"
      },
      "source": [
        "### Batch 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKqpsb94RB_A",
        "outputId": "de80f956-cbf0-4ce4-9685-c7b882ffe1aa"
      },
      "outputs": [],
      "source": [
        "# Extract the training set covariates\n",
        "train_covars = strat_covars[strat_covars['batch']!=4]\n",
        "\n",
        "# Filter by diagnosis\n",
        "age_diagnosis_0 = train_covars[train_covars['Dx'] == 0]['Age']\n",
        "age_diagnosis_1 = train_covars[train_covars['Dx'] == 1]['Age']\n",
        "\n",
        "# Create the plot\n",
        "sns.kdeplot(age_diagnosis_0, label='Healthy (Dx=0)', fill=True)\n",
        "sns.kdeplot(age_diagnosis_1, label='BD (Dx=1)', fill=True)\n",
        "plt.title('Age Distribution in Training Set')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "t_stat, p_value = ttest_ind(age_diagnosis_0, age_diagnosis_1, equal_var=False)\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQal1oUIRB_B",
        "outputId": "d9792589-94a0-45db-d4b3-4f0e5ac32c75"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define drop fractions\n",
        "drop_fraction_healthy = 0.3\n",
        "drop_fraction_bd = 0.3\n",
        "\n",
        "# Identify subjects to drop\n",
        "healthy_to_drop = train_covars[\n",
        "    (train_covars['Dx'] == 0) &\n",
        "    (train_covars['Age'] >= 15) &\n",
        "    (train_covars['Age'] <= 30)\n",
        "].sample(frac=drop_fraction_healthy, random_state=42)\n",
        "\n",
        "bd_to_drop = train_covars[\n",
        "    (train_covars['Dx'] == 1) &\n",
        "    (train_covars['Age'] >= 40) &\n",
        "    (train_covars['Age'] <= 60)\n",
        "].sample(frac=drop_fraction_bd, random_state=42)\n",
        "\n",
        "\n",
        "indices_to_remove_4 = set(healthy_to_drop.index).union(set(bd_to_drop.index))\n",
        "\n",
        "train_covars_new = train_covars.drop(indices_to_remove_4)\n",
        "\n",
        "\n",
        "print(f\"Removed {len(indices_to_remove_4)} subjects ({len(healthy_to_drop)} healthy, {len(bd_to_drop)} bd)\")\n",
        "print(f\"New training set size: {len(train_covars_new)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN4F4ejIRB_B",
        "outputId": "fd1d77b6-d62b-4be8-e35e-0e560890341a"
      },
      "outputs": [],
      "source": [
        "age_diagnosis_0 = train_covars_new[train_covars_new['Dx'] == 0]['Age']\n",
        "age_diagnosis_1 = train_covars_new[train_covars_new['Dx'] == 1]['Age']\n",
        "\n",
        "\n",
        "# Create the plot\n",
        "sns.kdeplot(age_diagnosis_0, label='Healthy (Dx=0)', fill=True)\n",
        "sns.kdeplot(age_diagnosis_1, label='BD (Dx=1)', fill=True)\n",
        "plt.title('Age Distribution in Training Set')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "t_stat, p_value = ttest_ind(age_diagnosis_0, age_diagnosis_1, equal_var=False)\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo1Whn93RYQh"
      },
      "source": [
        "### Batch 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-Be25G2RYQi",
        "outputId": "c249b6e8-1705-4190-b1ca-1390cb16674e"
      },
      "outputs": [],
      "source": [
        "# Extract the training set covariates\n",
        "train_covars = strat_covars[strat_covars['batch']!=5]\n",
        "\n",
        "# Filter by diagnosis\n",
        "age_diagnosis_0 = train_covars[train_covars['Dx'] == 0]['Age']\n",
        "age_diagnosis_1 = train_covars[train_covars['Dx'] == 1]['Age']\n",
        "\n",
        "# Create the plot\n",
        "sns.kdeplot(age_diagnosis_0, label='Healthy (Dx=0)', fill=True)\n",
        "sns.kdeplot(age_diagnosis_1, label='BD (Dx=1)', fill=True)\n",
        "plt.title('Age Distribution in Training Set')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "t_stat, p_value = ttest_ind(age_diagnosis_0, age_diagnosis_1, equal_var=False)\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2df9-BTRYQi",
        "outputId": "f145ebd8-34a4-4e9b-8ea4-232aef56efec"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define drop fractions\n",
        "drop_fraction_healthy = 0.2\n",
        "drop_fraction_bd = 0.2\n",
        "\n",
        "# Identify subjects to drop\n",
        "healthy_to_drop = train_covars[\n",
        "    (train_covars['Dx'] == 0) &\n",
        "    (train_covars['Age'] >= 15) &\n",
        "    (train_covars['Age'] <= 30)\n",
        "].sample(frac=drop_fraction_healthy, random_state=42)\n",
        "\n",
        "bd_to_drop = train_covars[\n",
        "    (train_covars['Dx'] == 1) &\n",
        "    (train_covars['Age'] >= 40) &\n",
        "    (train_covars['Age'] <= 60)\n",
        "].sample(frac=drop_fraction_bd, random_state=42)\n",
        "\n",
        "\n",
        "indices_to_remove_5 = set(healthy_to_drop.index).union(set(bd_to_drop.index))\n",
        "\n",
        "train_covars_new = train_covars.drop(indices_to_remove_5)\n",
        "\n",
        "\n",
        "print(f\"Removed {len(indices_to_remove_5)} subjects ({len(healthy_to_drop)} healthy, {len(bd_to_drop)} bd)\")\n",
        "print(f\"New training set size: {len(train_covars_new)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsk8NbECRYQi",
        "outputId": "9a13f764-f3b5-4996-fef8-c331b8d93509"
      },
      "outputs": [],
      "source": [
        "age_diagnosis_0 = train_covars_new[train_covars_new['Dx'] == 0]['Age']\n",
        "age_diagnosis_1 = train_covars_new[train_covars_new['Dx'] == 1]['Age']\n",
        "\n",
        "\n",
        "# Create the plot\n",
        "sns.kdeplot(age_diagnosis_0, label='Healthy (Dx=0)', fill=True)\n",
        "sns.kdeplot(age_diagnosis_1, label='BD (Dx=1)', fill=True)\n",
        "plt.title('Age Distribution in Training Set')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "t_stat, p_value = ttest_ind(age_diagnosis_0, age_diagnosis_1, equal_var=False)\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t__2houAR_xU"
      },
      "source": [
        "### Batch 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97H4Yb7nR_xU",
        "outputId": "8cd74fb1-6cce-4ed3-9eca-d574b77b7419"
      },
      "outputs": [],
      "source": [
        "# Extract the training set covariates\n",
        "train_covars = strat_covars[strat_covars['batch']!=6]\n",
        "\n",
        "# Filter by diagnosis\n",
        "age_diagnosis_0 = train_covars[train_covars['Dx'] == 0]['Age']\n",
        "age_diagnosis_1 = train_covars[train_covars['Dx'] == 1]['Age']\n",
        "\n",
        "# Create the plot\n",
        "sns.kdeplot(age_diagnosis_0, label='Healthy (Dx=0)', fill=True)\n",
        "sns.kdeplot(age_diagnosis_1, label='BD (Dx=1)', fill=True)\n",
        "plt.title('Age Distribution in Training Set')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "t_stat, p_value = ttest_ind(age_diagnosis_0, age_diagnosis_1, equal_var=False)\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5_3uQVYR_xV",
        "outputId": "620ecdbb-b473-4573-de3a-2ba07cf12497"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define drop fractions\n",
        "drop_fraction_healthy = 0.4\n",
        "drop_fraction_bd = 0.3\n",
        "\n",
        "# Identify subjects to drop\n",
        "healthy_to_drop = train_covars[\n",
        "    (train_covars['Dx'] == 0) &\n",
        "    (train_covars['Age'] >= 15) &\n",
        "    (train_covars['Age'] <= 30)\n",
        "].sample(frac=drop_fraction_healthy, random_state=42)\n",
        "\n",
        "bd_to_drop = train_covars[\n",
        "    (train_covars['Dx'] == 1) &\n",
        "    (train_covars['Age'] >= 40) &\n",
        "    (train_covars['Age'] <= 60)\n",
        "].sample(frac=drop_fraction_bd, random_state=42)\n",
        "\n",
        "\n",
        "indices_to_remove_6 = set(healthy_to_drop.index).union(set(bd_to_drop.index))\n",
        "\n",
        "train_covars_new = train_covars.drop(indices_to_remove_6)\n",
        "\n",
        "\n",
        "print(f\"Removed {len(indices_to_remove_6)} subjects ({len(healthy_to_drop)} healthy, {len(bd_to_drop)} bd)\")\n",
        "print(f\"New training set size: {len(train_covars_new)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAB2-qpKR_xV",
        "outputId": "52d38479-57c7-45d5-a598-b8d29f8276d4"
      },
      "outputs": [],
      "source": [
        "age_diagnosis_0 = train_covars_new[train_covars_new['Dx'] == 0]['Age']\n",
        "age_diagnosis_1 = train_covars_new[train_covars_new['Dx'] == 1]['Age']\n",
        "\n",
        "\n",
        "# Create the plot\n",
        "sns.kdeplot(age_diagnosis_0, label='Healthy (Dx=0)', fill=True)\n",
        "sns.kdeplot(age_diagnosis_1, label='BD (Dx=1)', fill=True)\n",
        "plt.title('Age Distribution in Training Set')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "t_stat, p_value = ttest_ind(age_diagnosis_0, age_diagnosis_1, equal_var=False)\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuH8gCbdS5AO"
      },
      "source": [
        "### Batch 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOOg61NiS5AP",
        "outputId": "dfb277af-d5a7-4baa-d329-91ccf7c10871"
      },
      "outputs": [],
      "source": [
        "# Extract the training set covariates\n",
        "train_covars = strat_covars[strat_covars['batch']!=7]\n",
        "\n",
        "# Filter by diagnosis\n",
        "age_diagnosis_0 = train_covars[train_covars['Dx'] == 0]['Age']\n",
        "age_diagnosis_1 = train_covars[train_covars['Dx'] == 1]['Age']\n",
        "\n",
        "# Create the plot\n",
        "sns.kdeplot(age_diagnosis_0, label='Healthy (Dx=0)', fill=True)\n",
        "sns.kdeplot(age_diagnosis_1, label='BD (Dx=1)', fill=True)\n",
        "plt.title('Age Distribution in Training Set')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "t_stat, p_value = ttest_ind(age_diagnosis_0, age_diagnosis_1, equal_var=False)\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHHeAi8DS5AQ",
        "outputId": "1cf85fb8-da58-463b-b483-ebca00096228"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define drop fractions\n",
        "drop_fraction_healthy = 0.5\n",
        "drop_fraction_bd = 0.4\n",
        "\n",
        "# Identify subjects to drop\n",
        "healthy_to_drop = train_covars[\n",
        "    (train_covars['Dx'] == 0) &\n",
        "    (train_covars['Age'] >= 15) &\n",
        "    (train_covars['Age'] <= 30)\n",
        "].sample(frac=drop_fraction_healthy, random_state=42)\n",
        "\n",
        "bd_to_drop = train_covars[\n",
        "    (train_covars['Dx'] == 1) &\n",
        "    (train_covars['Age'] >= 40) &\n",
        "    (train_covars['Age'] <= 60)\n",
        "].sample(frac=drop_fraction_bd, random_state=42)\n",
        "\n",
        "\n",
        "indices_to_remove_7 = set(healthy_to_drop.index).union(set(bd_to_drop.index))\n",
        "\n",
        "train_covars_new = train_covars.drop(indices_to_remove_7)\n",
        "\n",
        "\n",
        "print(f\"Removed {len(indices_to_remove_7)} subjects ({len(healthy_to_drop)} healthy, {len(bd_to_drop)} bd)\")\n",
        "print(f\"New training set size: {len(train_covars_new)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfqKV-emS5AQ",
        "outputId": "4a249642-dd12-45c5-f2f9-9eb356206913"
      },
      "outputs": [],
      "source": [
        "age_diagnosis_0 = train_covars_new[train_covars_new['Dx'] == 0]['Age']\n",
        "age_diagnosis_1 = train_covars_new[train_covars_new['Dx'] == 1]['Age']\n",
        "\n",
        "\n",
        "# Create the plot\n",
        "sns.kdeplot(age_diagnosis_0, label='Healthy (Dx=0)', fill=True)\n",
        "sns.kdeplot(age_diagnosis_1, label='BD (Dx=1)', fill=True)\n",
        "plt.title('Age Distribution in Training Set')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "t_stat, p_value = ttest_ind(age_diagnosis_0, age_diagnosis_1, equal_var=False)\n",
        "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "\n",
        "u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDZc-VwW5yKB"
      },
      "source": [
        "# Dataset and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gj3rYs7IcdAu"
      },
      "outputs": [],
      "source": [
        "# Dataset for Diagnosis Prediction\n",
        "class MindDataset(InMemoryDataset):\n",
        "    def __init__(self, root, matrices_harm, strat_covars, adjacency_matrices, roi_data, transform=None, pre_transform=None):\n",
        "        self.matrices_harm = matrices_harm\n",
        "        self.strat_covars = strat_covars\n",
        "        self.adjacency_matrices = adjacency_matrices\n",
        "        self.roi_data = roi_data\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "\n",
        "\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0], weights_only = False)\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return []\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "\n",
        "    def process(self):\n",
        "        data_list = []\n",
        "\n",
        "        for i in range(len(self.matrices_harm)):\n",
        "            # Node features: all elements in a row\n",
        "            node_features = torch.tensor(self.matrices_harm[i], dtype=torch.float)\n",
        "            roi_features = torch.tensor(self.roi_data.iloc[i].values, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "            gender = self.strat_covars['Gender'].iloc[i]\n",
        "            age = self.strat_covars['Age'].iloc[i]\n",
        "            age_gender_features = torch.tensor([[gender, age]] * self.matrices_harm[i].shape[0], dtype=torch.float)\n",
        "            node_features = torch.cat([node_features, age_gender_features, roi_features], dim=1)\n",
        "\n",
        "            edge_index = torch.tensor(np.array(np.where(self.adjacency_matrices[i] == 1)), dtype=torch.long)\n",
        "\n",
        "            edge_attr = []\n",
        "            for j in range(edge_index.shape[1]):\n",
        "                edge_attr.append(torch.tensor(self.matrices_harm[i][edge_index[0, j], edge_index[1, j]], dtype=torch.float))\n",
        "            edge_attr = torch.stack(edge_attr).unsqueeze(1)\n",
        "\n",
        "            # data = Data(x=node_features, edge_index=edge_index, y=torch.tensor(self.strat_covars['Dx'].iloc[i], dtype=torch.long))\n",
        "            data = Data(x=node_features, edge_index=edge_index, edge_attr = edge_attr, y=torch.tensor(self.strat_covars['Dx'].iloc[i], dtype=torch.long)) \n",
        "            data_list.append(data)\n",
        "\n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "    def len(self):\n",
        "        return super().len()\n",
        "\n",
        "    def get(self, idx):\n",
        "        return super().get(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuhNJfIm16sZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NodeNorm(nn.Module):\n",
        "    def __init__(self, nn_type=\"n\", unbiased=False, eps=1e-5, power_root=2):\n",
        "        super(NodeNorm, self).__init__()\n",
        "        self.unbiased = unbiased\n",
        "        self.eps = eps\n",
        "        self.nn_type = nn_type\n",
        "        self.power = 1 / power_root\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.nn_type == \"n\":\n",
        "            mean = torch.mean(x, dim=1, keepdim=True)\n",
        "            std = (\n",
        "                torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps\n",
        "            ).sqrt()\n",
        "            x = (x - mean) / std\n",
        "        elif self.nn_type == \"v\":\n",
        "            std = (\n",
        "                torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps\n",
        "            ).sqrt()\n",
        "            x = x / std\n",
        "        elif self.nn_type == \"m\":\n",
        "            mean = torch.mean(x, dim=1, keepdim=True)\n",
        "            x = x - mean\n",
        "        elif self.nn_type == \"srv\":  # square root of variance\n",
        "            std = (\n",
        "                torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps\n",
        "            ).sqrt()\n",
        "            x = x / torch.sqrt(std)\n",
        "        elif self.nn_type == \"pr\":\n",
        "            std = (\n",
        "                torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps\n",
        "            ).sqrt()\n",
        "            x = x / torch.pow(std, self.power)\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        original_str = super().__repr__()\n",
        "        components = list(original_str)\n",
        "        nn_type_str = f\"nn_type={self.nn_type}\"\n",
        "        components.insert(-1, nn_type_str)\n",
        "        new_str = \"\".join(components)\n",
        "        return new_str\n",
        "\n",
        "def get_normalization(norm_type, num_channels=None):\n",
        "    if norm_type is None:\n",
        "        norm = None\n",
        "    elif norm_type == \"batch\":\n",
        "        norm = nn.BatchNorm1d(num_features=num_channels)\n",
        "    elif norm_type == \"node_n\":\n",
        "        norm = NodeNorm(nn_type=\"n\")\n",
        "    elif norm_type == \"node_v\":\n",
        "        norm = NodeNorm(nn_type=\"v\")\n",
        "    elif norm_type == \"node_m\":\n",
        "        norm = NodeNorm(nn_type=\"m\")\n",
        "    elif norm_type == \"node_srv\":\n",
        "        norm = NodeNorm(nn_type=\"srv\")\n",
        "    elif norm_type.find(\"node_pr\") != -1:\n",
        "        power_root = norm_type.split(\"_\")[-1]\n",
        "        power_root = int(power_root)\n",
        "        norm = NodeNorm(nn_type=\"pr\", power_root=power_root)\n",
        "    elif norm_type == \"layer\":\n",
        "        norm = nn.LayerNorm(normalized_shape=num_channels)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o236Y8nj2JFg"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, norm_type=\"node_n\"):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "\n",
        "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
        "        self.bns.append(get_normalization(norm_type=norm_type, num_channels=hidden_dim))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "            self.bns.append(get_normalization(norm_type=norm_type, num_channels=hidden_dim))\n",
        "\n",
        "        self.fc = Linear(hidden_dim, output_dim)\n",
        "        # self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = bn(x)\n",
        "            # x = self.dropout(x)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNvdd4RIM0TT",
        "outputId": "c91a7fc5-9ed8-43b2-8dda-657582ff6024"
      },
      "outputs": [],
      "source": [
        "import Confounder_Correction_Classes\n",
        "from Confounder_Correction_Classes import ComBatHarmonization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = F.cross_entropy(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        all_preds.append(out.argmax(dim=1).cpu().numpy())\n",
        "        all_labels.append(data.y.cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    return accuracy, train_loss, f1\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.cross_entropy(out, data.y)\n",
        "            total_loss += loss.item()\n",
        "            all_preds.append(out.argmax(dim=1).cpu().numpy())\n",
        "            all_labels.append(data.y.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "    f1 = f1_score(all_labels, all_preds, zero_division=1.0)\n",
        "    val_loss = total_loss / len(loader)\n",
        "\n",
        "    return accuracy, val_loss, f1, all_labels, all_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZpLXfdmWBs9"
      },
      "outputs": [],
      "source": [
        "def split_data(data, strat_covars, roi_data, indices_1, indices_2):\n",
        "\n",
        "    data_1 = data.drop(indices_2).reset_index(drop=True)\n",
        "    strat_covars_1 = strat_covars.drop(indices_2).reset_index(drop=True)\n",
        "    roi_1 = roi_data.drop(indices_2).reset_index(drop=True)\n",
        "\n",
        "    data_2 = data.drop(indices_1).reset_index(drop=True)\n",
        "    strat_covars_2 = strat_covars.drop(indices_1).reset_index(drop=True)\n",
        "    roi_2 = roi_data.drop(indices_1).reset_index(drop=True)\n",
        "\n",
        "    return data_1, strat_covars_1, roi_1, data_2, strat_covars_2, roi_2\n",
        "\n",
        "def reconstruct_and_create_adjacency(data_harm, threshold_percentile=70):\n",
        "    n_matrices, upper_triangle_size = data_harm.shape\n",
        "    N = int((1 + np.sqrt(1 + 8 * upper_triangle_size)) // 2)\n",
        "    matrices = np.zeros((n_matrices, N, N))\n",
        "    adjacency_matrices = np.zeros((n_matrices, N, N), dtype=int)\n",
        "\n",
        "    for i in range(n_matrices):\n",
        "        matrix = np.eye(N)\n",
        "        upper_indices = np.triu_indices(N, k=1)\n",
        "        matrix[upper_indices] = data_harm.iloc[i]\n",
        "        matrix = matrix + matrix.T\n",
        "        np.fill_diagonal(matrix, 1)\n",
        "        matrices[i] = matrix\n",
        "\n",
        "        threshold = np.percentile(matrix, threshold_percentile)\n",
        "        adjacency_matrices[i] = (matrix >= threshold).astype(int)\n",
        "\n",
        "    return matrices, adjacency_matrices\n",
        "\n",
        "def harmonize_data_2(data_1_raw, strat_covars_1, data_2_raw, strat_covars_2, ext_batch):\n",
        "    volumes_columns = np.arange(0, data_1_raw.shape[1])\n",
        "\n",
        "    feat_detail={'volumes':           {'id': volumes_columns,\n",
        "                                            'categorical': ['Gender'],\n",
        "                                            'continuous':['Age']}}\n",
        "\n",
        "    combat_function=ComBatHarmonization(cv_method=None, ref_batch=None,\n",
        "                                           regression_fit=0,\n",
        "                                           feat_detail=feat_detail,\n",
        "                                           feat_of_no_interest=None)\n",
        "\n",
        "    data_1_dict={'data': data_1_raw, 'covariates': strat_covars_1}\n",
        "    data_1_harm = combat_function.fit_transform(data_1_dict)\n",
        "    data_1_harm = pd.DataFrame(data_1_harm)\n",
        "\n",
        "    #Harmonize the test set\n",
        "    data_2_raw = pd.DataFrame(data_2_raw)\n",
        "    data_2_raw.columns = data_2_raw.columns.astype(int)\n",
        "    data_2_raw.index = pd.RangeIndex(start=0, stop=len(data_2_raw), step=1)\n",
        "    all_data = pd.concat([data_1_harm, data_2_raw], ignore_index=True)\n",
        "    all_strat_covars = pd.concat([strat_covars_1, strat_covars_2], ignore_index=True)\n",
        "    all_strat_covars.loc[all_strat_covars[\"batch\"] != ext_batch, \"batch\"] = 0\n",
        "    \n",
        "    test_combat_function=ComBatHarmonization(cv_method=None, ref_batch=0,\n",
        "                                           regression_fit=0,\n",
        "                                           feat_detail=feat_detail,\n",
        "                                           feat_of_no_interest=None)\n",
        "\n",
        "    all_data_dict={'data': all_data, 'covariates': all_strat_covars}\n",
        "    all_data_harm = test_combat_function.fit_transform(all_data_dict)\n",
        "    all_data_harm = pd.DataFrame(all_data_harm)\n",
        "\n",
        "    data_2_harm = all_data_harm.drop(data_1_harm.index)\n",
        "    data_2_harm = data_2_harm.reset_index(drop=True)\n",
        "    data_2_harm = pd.DataFrame(data_2_harm)\n",
        "\n",
        "    return data_1_harm, data_2_harm\n",
        "\n",
        "def harmonize_data(train_data_raw, train_strat_covars, test_data_raw, test_strat_covars):\n",
        "\n",
        "            volumes_columns = np.arange(0, train_data_raw.shape[1])\n",
        "\n",
        "            feat_detail={'volumes':           {'id': volumes_columns,\n",
        "                                            'categorical': ['Gender', 'Dx'],\n",
        "                                            'continuous':['Age']}}\n",
        "\n",
        "            combat_function=ComBatHarmonization(cv_method=None, ref_batch=None,\n",
        "                                              regression_fit=0,\n",
        "                                              feat_detail=feat_detail,\n",
        "                                              feat_of_no_interest=None)\n",
        "\n",
        "            train_data_dict={'data': train_data_raw, 'covariates': train_strat_covars}\n",
        "            train_data_harm = combat_function.fit_transform(train_data_dict)\n",
        "            train_data_harm = pd.DataFrame(train_data_harm)\n",
        "\n",
        "            test_data_dict={'data': test_data_raw, 'covariates': test_strat_covars}\n",
        "            test_data_harm = combat_function.transform(test_data_dict)\n",
        "            test_data_harm = pd.DataFrame(test_data_harm)\n",
        "\n",
        "            return train_data_harm, test_data_harm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Edge Weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from torch.nn import BatchNorm1d, Dropout\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, norm_type=\"node_n\"):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "\n",
        "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
        "        self.bns.append(get_normalization(norm_type=norm_type, num_channels=hidden_dim))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "            self.bns.append(get_normalization(norm_type=norm_type, num_channels=hidden_dim))\n",
        "\n",
        "        self.fc = Linear(hidden_dim, output_dim)\n",
        "        # self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = F.relu(conv(x, edge_index, edge_attr))\n",
        "            x = bn(x)\n",
        "            # x = self.dropout(x)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "    \n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        loss = F.cross_entropy(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        all_preds.append(out.argmax(dim=1).cpu().numpy())\n",
        "        all_labels.append(data.y.cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
        "    train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    return accuracy, train_loss, f1\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "            loss = F.cross_entropy(out, data.y)\n",
        "            total_loss += loss.item()\n",
        "            all_preds.append(out.argmax(dim=1).cpu().numpy())\n",
        "            all_labels.append(data.y.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
        "    val_loss = total_loss / len(loader)\n",
        "\n",
        "    return accuracy, val_loss, f1, all_preds, all_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNHZQM3H3k7f"
      },
      "source": [
        "# K-Fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jxM3xXT3kfw",
        "outputId": "d92ee3fa-eb89-401b-c117-894a9ade3c61"
      },
      "outputs": [],
      "source": [
        "ext_test_batches = [1, 3, 4, 5, 6, 7]\n",
        "indices_to_remove = [indices_to_remove_1, indices_to_remove_3, indices_to_remove_4, indices_to_remove_5, indices_to_remove_6, indices_to_remove_7] #\n",
        "final_results = []\n",
        "\n",
        "for ext_test_batch, indices in zip(ext_test_batches, indices_to_remove):\n",
        "    print(f\"Test batch: {ext_test_batch}\")\n",
        "\n",
        "    data_raw = pd.read_csv('data_raw_nmm_new.csv')\n",
        "    strat_covars = pd.read_csv('MatchedData01.csv')\n",
        "    roi_raw = pd.read_csv('roi_data_raw_nmm_norm.csv')\n",
        "\n",
        "    data_raw = data_raw.drop(indices)\n",
        "    strat_covars = strat_covars.drop(indices)\n",
        "    roi_raw = roi_raw.drop(indices)\n",
        "\n",
        "    #SEPARATE CROSS VALIDATION DATA AND EXTERNAL TEST SET DATA\n",
        "    ext_test_indices = strat_covars[strat_covars['batch'] == ext_test_batch].index\n",
        "    cv_indices = strat_covars[strat_covars[\"batch\"] != ext_test_batch].index\n",
        "\n",
        "    cv_data_raw, cv_strat_covars, cv_roi_raw, ext_test_data_raw, ext_test_strat_covars, ext_test_roi_raw = split_data(data_raw, strat_covars, roi_raw, cv_indices, ext_test_indices)\n",
        "\n",
        "    k_folds = 5\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    #CROSS VALIDATION\n",
        "\n",
        "    for fold, (train_indices, test_indices) in enumerate(kf.split(cv_data_raw)):\n",
        "        print(f\"\\nFold: {fold+1}/{k_folds}\")\n",
        "\n",
        "        #SEPARATE TRAIN AND TEST DATA\n",
        "\n",
        "        train_data_raw, train_strat_covars, train_roi_raw, test_data_raw, test_strat_covars, test_roi_raw = split_data(cv_data_raw, cv_strat_covars, cv_roi_raw, train_indices, test_indices)\n",
        "\n",
        "        # HARMONIZE DATA\n",
        "        train_data_harm, test_data_harm = harmonize_data(train_data_raw, train_strat_covars, test_data_raw, test_strat_covars)\n",
        "        train_roi_harm, test_roi_harm = harmonize_data(train_roi_raw, train_strat_covars, test_roi_raw, test_strat_covars)\n",
        "\n",
        "        print(\"Data harmonized\")\n",
        "\n",
        "        # RECONSTRUCT MATRICES AND CREATE ADJACENCY MATRICES\n",
        "\n",
        "        train_matrices_harm, train_adjacency_matrices = reconstruct_and_create_adjacency(train_data_harm)\n",
        "        test_matrices_harm, test_adjacency_matrices = reconstruct_and_create_adjacency(test_data_harm)\n",
        "\n",
        "        print(\"Matrices created\")\n",
        "\n",
        "        # CREATE DATASETS\n",
        "\n",
        "        train_dataset_path = f\"Datasets/CV/NMM70/MindDatasetTrainNMM70_{ext_test_batch}_fold{fold+1}\"\n",
        "        test_dataset_path = f\"Datasets/CV/NMM70/MindDatasetValNMM70_{ext_test_batch}_fold{fold+1}\"\n",
        "\n",
        "        train_dataset = MindDataset(\n",
        "            root=train_dataset_path,\n",
        "            matrices_harm=train_matrices_harm,\n",
        "            strat_covars=train_strat_covars,\n",
        "            adjacency_matrices=train_adjacency_matrices,\n",
        "            roi_data=train_roi_harm\n",
        "        )\n",
        "\n",
        "        test_dataset = MindDataset(\n",
        "            root=test_dataset_path,\n",
        "            matrices_harm=test_matrices_harm,\n",
        "            strat_covars=test_strat_covars,\n",
        "            adjacency_matrices=test_adjacency_matrices,\n",
        "            roi_data=test_roi_harm\n",
        "        )\n",
        "\n",
        "        #TRAIN AND TEST DIFFERENT PARAMETERS\n",
        "\n",
        "        layer_dim_pairs = [(2, 32), (2,64),(2,128)]\n",
        "        norm_types_list = [\"node_n\", \"node_m\", \"node_srv\", \"node_pr_2\"]\n",
        "        batch_size = [16, 32]\n",
        "        param_grid = [(nl, hd, norm, bat) for (nl, hd) in layer_dim_pairs for norm in norm_types_list for bat in batch_size]\n",
        "\n",
        "        best_val_acc = 0\n",
        "        best_params = None\n",
        "        best_model_path = \"best_grid_model_1.pt\"\n",
        "\n",
        "        for num_layers, hidden_dim, norm_type, batch_size in param_grid:\n",
        "            print(f\"\\nTraining with num_layers={num_layers}, hidden_dim={hidden_dim}, norm_type={norm_type}, batch_size={batch_size}\")\n",
        "\n",
        "            train_indices, val_indices = train_test_split(range(len(train_dataset)), test_size=0.3, random_state=3)\n",
        "            train_loader = DataLoader(train_dataset[train_indices], batch_size=batch_size, shuffle=True)\n",
        "            val_loader = DataLoader(train_dataset[val_indices], batch_size=batch_size, shuffle=False)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            set_seed(42)\n",
        "\n",
        "            model = GCN(train_dataset.num_features, hidden_dim=hidden_dim, output_dim=train_dataset.num_classes, num_layers=num_layers, norm_type=norm_type)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "            patience = 20\n",
        "            best_val_loss = float(\"inf\")\n",
        "            patience_counter = 0\n",
        "\n",
        "            for epoch in range(100):\n",
        "                train_acc, train_loss, train_f1 = train()\n",
        "                val_acc, val_loss, val_f1, _, _ = evaluate(val_loader)\n",
        "\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    patience_counter = 0\n",
        "                    torch.save(model.state_dict(), \"temp_model_1.pt\")  # Save the best model for this config\n",
        "                    print(f\"Saved best model at epoch {epoch} with  Train Accuracy: {train_acc:.4f}, Val Accuracy {val_acc:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch}\")\n",
        "                    break\n",
        "\n",
        "            # Load best model for this configuration\n",
        "            model.load_state_dict(torch.load(\"temp_model_1.pt\"))\n",
        "            test_acc, test_loss, test_f1,_,_ = evaluate(test_loader)\n",
        "\n",
        "            print(f\"For num_layers={num_layers}, hidden_dim={hidden_dim}, norm_type={norm_type}, batch:size={batch_size}: Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}, Test F1: {test_f1:.4f}\")\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                \"fold\": fold + 1,\n",
        "                \"num_layers\": num_layers,\n",
        "                \"hidden_dim\": hidden_dim,\n",
        "                \"norm_type\": norm_type,\n",
        "                \"batch_size\": batch_size,\n",
        "                \"test_accuracy\": test_acc,\n",
        "                \"test_loss\": test_loss,\n",
        "                \"test_f1\": test_f1\n",
        "            })\n",
        "\n",
        "    df_results = pd.DataFrame(results)\n",
        "\n",
        "    df_mean_results = df_results.groupby([\"num_layers\", \"hidden_dim\", \"norm_type\", \"batch_size\"]).agg(\n",
        "    {\"test_accuracy\": \"mean\", \"test_loss\": \"mean\", \"test_f1\": \"mean\"}).reset_index()\n",
        "\n",
        "     # Find the row with the highest mean test accuracy\n",
        "    best_row = df_mean_results.loc[df_mean_results[\"test_accuracy\"].idxmax()]\n",
        "\n",
        "    # Extract the best parameters\n",
        "    best_num_layers = int(best_row[\"num_layers\"])\n",
        "    best_hidden_dim = int(best_row[\"hidden_dim\"])\n",
        "    best_norm_type = best_row[\"norm_type\"]\n",
        "    best_batch_size = int(best_row[\"batch_size\"])\n",
        "\n",
        "    print(f\"Best parameters: num_layers={best_num_layers}, hidden_dim={best_hidden_dim}, norm_type={best_norm_type}, batch_size={best_batch_size}\")\n",
        "\n",
        "    cv_data_harm, ext_test_data_harm = harmonize_data_2(cv_data_raw, cv_strat_covars, ext_test_data_raw, ext_test_strat_covars, ext_test_batch)\n",
        "    cv_roi_harm, ext_test_roi_harm = harmonize_data_2(cv_roi_raw, cv_strat_covars, ext_test_roi_raw, ext_test_strat_covars, ext_test_batch)\n",
        "\n",
        "    ext_test_matrices_harm, ext_test_adjacency_matrices = reconstruct_and_create_adjacency(ext_test_data_harm)\n",
        "\n",
        "    ext_test_dataset_path = f\"Datasets/CV/NMM70/MindDatasetExtTestNMM70_batch{ext_test_batch}\"\n",
        "    ext_test_dataset = MindDataset(\n",
        "        root=ext_test_dataset_path,\n",
        "        matrices_harm=ext_test_matrices_harm,\n",
        "        strat_covars=ext_test_strat_covars,\n",
        "        adjacency_matrices=ext_test_adjacency_matrices,\n",
        "        roi_data=ext_test_roi_harm\n",
        "    )\n",
        "\n",
        "    cv_matrices_harm, cv_adjacency_matrices = reconstruct_and_create_adjacency(cv_data_harm)\n",
        "    cv_dataset_path = f\"Datasets/CV/NMM70/MindDatasetCVNMM70_batch{ext_test_batch}\"\n",
        "    cv_dataset = MindDataset(\n",
        "        root=cv_dataset_path,\n",
        "        matrices_harm=cv_matrices_harm,\n",
        "        strat_covars=cv_strat_covars,\n",
        "        adjacency_matrices=cv_adjacency_matrices,\n",
        "        roi_data=cv_roi_harm\n",
        "    )\n",
        "    train_indices, val_indices = train_test_split(range(len(cv_dataset)), test_size=0.3, random_state=3)\n",
        "    train_loader = DataLoader(cv_dataset[train_indices], batch_size=best_batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(cv_dataset[val_indices], batch_size=best_batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(ext_test_dataset, batch_size=best_batch_size, shuffle=False)\n",
        "\n",
        "    set_seed(42)\n",
        "\n",
        "    model = GCN(cv_dataset.num_features, hidden_dim=best_hidden_dim, output_dim=cv_dataset.num_classes, num_layers=best_num_layers, norm_type=best_norm_type)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    patience = 20\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_val_f1 = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(100):\n",
        "\n",
        "        train_acc, train_loss, train_f1 = train()\n",
        "        val_acc, val_loss, val_f1, _, _ = evaluate(val_loader)  # Validation set metrics\n",
        "        print(\n",
        "            f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Train F1: {train_f1:.4f}, \"\n",
        "            f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}, Val F1: {val_f1:.4f}\"\n",
        "        )\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"best_model_1.pt\")\n",
        "            print(f\"Saved best model at epoch {epoch} with Val Loss: {val_loss:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch} as validation loss did not improve for {patience} consecutive epochs.\")\n",
        "            break\n",
        "\n",
        "\n",
        "    # Evaluate on test set after training is complete\n",
        "    model.load_state_dict(torch.load(\"best_model_1.pt\"))\n",
        "    test_acc, test_loss, test_f1, all_labels, all_preds = evaluate(test_loader)\n",
        "\n",
        "    test_prec = precision_score(all_labels, all_preds, zero_division=1.0)\n",
        "    test_recall = recall_score(all_labels, all_preds, zero_division=1.0)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
        "\n",
        "    final_results.append({\n",
        "                \"batch\": ext_test_batch,\n",
        "                \"num_layers\": best_num_layers,\n",
        "                \"hidden_dim\": best_hidden_dim,\n",
        "                \"norm_type\": best_norm_type,\n",
        "                \"batch_size\": best_batch_size,\n",
        "                \"accuracy\": test_acc*100,\n",
        "                \"loss\": test_loss,\n",
        "                \"f1\": test_f1*100,\n",
        "                \"precision\": test_prec*100,\n",
        "                \"recall\": test_recall*100,\n",
        "                \"true_negative\": tn,\n",
        "                \"false_positive\": fp,\n",
        "                \"false_negative\": fn,\n",
        "                \"true_positive\": tp\n",
        "            })\n",
        "\n",
        "    print(f\"External test {ext_test_batch} Accuracy: {test_acc:.4f}, Test F1: {test_f1:.4f}, Precision:{test_prec:.4f}, Recall:{test_recall:.4f}, True Negative {tn}, False Positive {fp}, False Negative {fn}, True Positive {tp}\")\n",
        "\n",
        "df_final_results = pd.DataFrame(final_results)\n",
        "df_final_results.to_csv('cv_final_results.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Ek88DZVppIJU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

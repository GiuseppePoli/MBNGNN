{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "4V_mqdC8yY2C",
        "outputId": "0d45d392-a9bd-4048-bbb3-22737e7e1ece"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import scipy.io\n",
        "import h5py\n",
        "import sklearn\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import ttest_ind, mannwhitneyu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AipOk_gaf4HL"
      },
      "source": [
        "# **Load data and reconstruct matrices**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQap4u7ggHsi"
      },
      "outputs": [],
      "source": [
        "data_raw = pd.read_csv('data_raw_nmm_new.csv')\n",
        "data_harm = pd.read_csv('data_harm_nmm_new.csv')\n",
        "roi_data_harm = pd.read_csv('roi_data_harm_nmm_new.csv')\n",
        "strat_covars = pd.read_csv('MatchedData01.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew6eumUufpZJ",
        "outputId": "85a3e356-adac-4c05-9c6c-8104852bd750"
      },
      "outputs": [],
      "source": [
        "#reconstruct the matrices from harmonized data\n",
        "\n",
        "n_matrices, upper_triangle_size = data_harm.shape\n",
        "N = int((1 + np.sqrt(1 + 8 * upper_triangle_size)) // 2)\n",
        "matrices_harm = np.zeros((n_matrices, N, N))\n",
        "\n",
        "for i in range(n_matrices):\n",
        "  matrix = np.eye(N)\n",
        "  upper_indices = np.triu_indices(N, k=1)\n",
        "  matrix[upper_indices] = data_harm.iloc[i]\n",
        "  matrix = matrix + matrix.T\n",
        "  np.fill_diagonal(matrix, 1)\n",
        "  matrices_harm[i] = matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyV7PdR9en8X"
      },
      "outputs": [],
      "source": [
        "# Create adjacency matrices from harmonized data\n",
        "adjacency_matrices = np.zeros_like(matrices_harm, dtype=int)\n",
        "for i in range(matrices_harm.shape[0]):\n",
        "  threshold = np.percentile(matrices_harm[i], 70)\n",
        "  adjacency_matrices[i] = (matrices_harm[i] >= threshold).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZzRGVrulu73"
      },
      "outputs": [],
      "source": [
        "#Create masked matrices\n",
        "masked_matrices_harm = matrices_harm * adjacency_matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuUr_v7HQ_eG"
      },
      "source": [
        "# **PyTorch Geometric**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0tEv0WyjV-Z",
        "outputId": "d6b3acb4-621c-4c5b-cadd-8b27f5624b97"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data, Dataset, InMemoryDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gj3rYs7IcdAu"
      },
      "outputs": [],
      "source": [
        "class MindDataset(InMemoryDataset):\n",
        "    def __init__(self, root, matrices_harm, strat_covars, adjacency_matrices, roi_data, transform=None, pre_transform=None):\n",
        "        self.roi_data = roi_data\n",
        "        self.matrices_harm = matrices_harm\n",
        "        self.strat_covars = strat_covars\n",
        "        self.adjacency_matrices = adjacency_matrices\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "\n",
        "\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0], weights_only = False)\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return []\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "\n",
        "    def process(self):\n",
        "        data_list = []\n",
        "\n",
        "        for i in range(len(matrices_harm)):\n",
        "            # Node features: all elements in a row\n",
        "            node_features = torch.tensor(self.matrices_harm[i], dtype=torch.float)\n",
        "            roi_features = torch.tensor(self.roi_data.iloc[i].values, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "            \n",
        "            age = self.strat_covars['Age'].iloc[i]\n",
        "            age_gender_features = torch.tensor([[age]] * self.matrices_harm[i].shape[0], dtype=torch.float)\n",
        "            node_features = torch.cat([node_features, age_gender_features, roi_features], dim=1)\n",
        "\n",
        "            edge_index = torch.tensor(np.array(np.where(self.adjacency_matrices[i] == 1)), dtype=torch.long)\n",
        "\n",
        "            edge_attr = []\n",
        "            for j in range(edge_index.shape[1]):\n",
        "                edge_attr.append(torch.tensor(self.matrices_harm[i][edge_index[0, j], edge_index[1, j]], dtype=torch.float))\n",
        "            edge_attr = torch.stack(edge_attr).unsqueeze(1)\n",
        "\n",
        "            data = Data(x=node_features, edge_index=edge_index, edge_attr = edge_attr, y=torch.tensor(self.strat_covars['Dx'].iloc[i], dtype=torch.long))\n",
        "            # data = Data(x=node_features, edge_index=edge_index, y=torch.tensor(self.strat_covars['Dx'].iloc[i], dtype=torch.long))\n",
        "            data_list.append(data)\n",
        "\n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "    def len(self):\n",
        "        return super().len()\n",
        "\n",
        "    def get(self, idx):\n",
        "        return super().get(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_root = f'Datasets/MindDatasetNMM70'\n",
        "\n",
        "dataset = MindDataset(\n",
        "    root=dataset_root,\n",
        "    matrices_harm=matrices_harm,\n",
        "    strat_covars=strat_covars,\n",
        "    adjacency_matrices=adjacency_matrices,\n",
        "    roi_data=roi_data_harm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzifupSrp-1Y",
        "outputId": "9250cca5-db06-4bf7-e9b0-cb791598d8d5"
      },
      "outputs": [],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPHWo7YTbxtn",
        "outputId": "483d2030-2aff-4f6c-c656-67ca7ea5a008"
      },
      "outputs": [],
      "source": [
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAUMxMr4h2xK",
        "outputId": "d0236331-e408-43c7-84fc-05cdfc091fa8"
      },
      "outputs": [],
      "source": [
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('=============================================================')\n",
        "\n",
        "# Gather some statistics about the first graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "id": "shPKm5GBqzrG",
        "outputId": "6f814801-c8dd-4afb-9a35-7bcd01f4f077"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.axis('off')\n",
        "\n",
        "node_colors = [data.y.item()] * data.num_nodes\n",
        "\n",
        "nx.draw_networkx(G,\n",
        "                pos=nx.spring_layout(G, seed=0),\n",
        "                with_labels=True,\n",
        "                node_size=800,\n",
        "                node_color=node_colors,\n",
        "                cmap=\"hsv\",\n",
        "                vmin=-2,\n",
        "                vmax=3,\n",
        "                width=0.8,\n",
        "                edge_color=\"grey\",\n",
        "                font_size=14\n",
        "                )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6h1Rri7T_7c"
      },
      "source": [
        "# **Train-validation-test split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RonaU2trN3OW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_indices, temp_indices = train_test_split(range(len(dataset)), test_size=0.3, random_state=3)\n",
        "val_indices, test_indices = train_test_split(temp_indices, test_size=0.5, random_state=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aljSLkD75QvG",
        "outputId": "cd9523ee-9d16-499b-87f8-1d5aa9fdc2e3"
      },
      "outputs": [],
      "source": [
        "# Extract the training set covariates\n",
        "# train_covars = strat_covars.iloc[train_indices]\n",
        "\n",
        "# Filter by diagnosis\n",
        "healthy_ages = strat_covars[strat_covars['Dx'] == 0]['Age']\n",
        "bd_ages = strat_covars[strat_covars['Dx'] == 1]['Age']\n",
        "\n",
        "# Create the plot\n",
        "sns.kdeplot(healthy_ages, label='Healthy', fill=True)\n",
        "sns.kdeplot(bd_ages, label='BD', fill=True)\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.savefig('age_distribution_pre.png', dpi = 300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r39RiuLPLYU",
        "outputId": "b6b83852-d10b-4637-fad2-5f4b2bd5de8e"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "age_diagnosis_0 = strat_covars[strat_covars['Dx'] == 0]['Age']\n",
        "age_diagnosis_1 = strat_covars[strat_covars['Dx'] == 1]['Age']\n",
        "\n",
        "u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "\n",
        "print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axfNKGivUfaC",
        "outputId": "2cd8435f-48cb-44fd-f7fe-1e7fd8655104"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define drop fractions\n",
        "drop_fraction_healthy = 0.3\n",
        "drop_fraction_bd = 0.3\n",
        "\n",
        "train_covars = strat_covars.iloc[train_indices]\n",
        "\n",
        "# Identify subjects to drop\n",
        "healthy_to_drop = train_covars[\n",
        "    (train_covars['Dx'] == 0) &\n",
        "    (train_covars['Age'] >= 15) &\n",
        "    (train_covars['Age'] <= 30)\n",
        "].sample(frac=drop_fraction_healthy, random_state=42)\n",
        "\n",
        "bd_to_drop = train_covars[\n",
        "    (train_covars['Dx'] == 1) &\n",
        "    (train_covars['Age'] >= 40) &\n",
        "    (train_covars['Age'] <= 60)\n",
        "].sample(frac=drop_fraction_bd, random_state=42)\n",
        "\n",
        "\n",
        "indices_to_remove = set(healthy_to_drop.index).union(set(bd_to_drop.index))\n",
        "\n",
        "# Create the new train indices excluding these subjects\n",
        "train_indices_new = [idx for idx in train_indices if idx not in indices_to_remove]\n",
        "\n",
        "\n",
        "print(f\"Removed {len(indices_to_remove)} subjects.\")\n",
        "print(f\"New training set size: {len(train_indices_new)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc_RHvBKUkPW",
        "outputId": "54ed8f90-5cdb-4a7f-d8ac-3042ddd6bbdf"
      },
      "outputs": [],
      "source": [
        "# Extract the training set covariates\n",
        "train_covars_new = strat_covars.iloc[train_indices_new]\n",
        "\n",
        "# Filter by diagnosis\n",
        "healthy_ages = train_covars_new[train_covars_new['Dx'] == 0]['Age']\n",
        "bd_ages = train_covars_new[train_covars_new['Dx'] == 1]['Age']\n",
        "\n",
        "# Create the plot\n",
        "sns.kdeplot(healthy_ages, label='Healthy', fill=True)\n",
        "sns.kdeplot(bd_ages, label='BD', fill=True)\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.savefig('age_distribution_post.png', dpi = 300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9q3sAy0PVz2",
        "outputId": "68876962-1ae0-482f-9642-dcdb2cae48d6"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "age_diagnosis_0 = train_covars_new[train_covars_new['Dx'] == 0]['Age']\n",
        "age_diagnosis_1 = train_covars_new[train_covars_new['Dx'] == 1]['Age']\n",
        "\n",
        "u_stat, p_value = mannwhitneyu(age_diagnosis_0, age_diagnosis_1, alternative='two-sided')\n",
        "\n",
        "print(f\"U-statistic: {u_stat}, P-value: {p_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNIksO1lZI1_"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset[train_indices_new], batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(dataset[val_indices], batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(dataset[test_indices], batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H624ZunON3OY",
        "outputId": "8ebf6c0f-32d7-4298-94d2-76c620b6c618"
      },
      "outputs": [],
      "source": [
        "counts = torch.bincount(train_loader.dataset.y)\n",
        "print(counts)\n",
        "print(counts[0]/sum(counts))\n",
        "counts = torch.bincount(val_loader.dataset.y)\n",
        "print(counts)\n",
        "print(counts[0]/sum(counts))\n",
        "counts = torch.bincount(test_loader.dataset.y)\n",
        "print(counts)\n",
        "print(counts[0]/sum(counts))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP4kI8B_iEEI"
      },
      "source": [
        "# **Graph Neural Networks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXUS7sBhiEEI"
      },
      "outputs": [],
      "source": [
        "#Graph Convolutional Network\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch import Tensor\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import random\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTDyohdO9LqJ"
      },
      "source": [
        "### GAT + BatchNorm and DropOut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoJT9QEjBktN"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GATConv, global_add_pool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmjcBXwY9TXI",
        "outputId": "fb49fe73-b0dd-4f5f-cc37-d692d46c2286"
      },
      "outputs": [],
      "source": [
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, heads=4, dropout=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        # Input layer\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n",
        "        self.bns.append(BatchNorm1d(hidden_channels * heads))  # Adjusted for multi-head output\n",
        "\n",
        "        # Hidden layers\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n",
        "            self.bns.append(BatchNorm1d(hidden_channels * heads))\n",
        "\n",
        "        # Output layer (single head for classification)\n",
        "        self.conv_out = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        batch_size = batch.max().item() + 1\n",
        "\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = conv(x, edge_index).relu()\n",
        "            x = bn(x)  # Apply Batch Normalization\n",
        "            x = self.dropout(x)  # Apply Dropout\n",
        "\n",
        "        x = self.conv_out(x, edge_index)\n",
        "        x = global_add_pool(x, batch, size=batch_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = GAT(dataset.num_features, 32, dataset.num_classes, num_layers=3)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2nF6_kl0lnk"
      },
      "source": [
        "### GIN + BatchNorm and DropOut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNuci1YU0ted",
        "outputId": "e1abf893-8683-4de2-a2dc-8f3dcf654e3b"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import MLP, GINConv, global_add_pool\n",
        "from torch.nn import BatchNorm1d\n",
        "\n",
        "class GIN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            mlp = MLP([in_channels, hidden_channels, hidden_channels])\n",
        "            self.convs.append(GINConv(nn=mlp, train_eps=False))\n",
        "            self.bns.append(BatchNorm1d(hidden_channels))\n",
        "            in_channels = hidden_channels\n",
        "\n",
        "        self.mlp = MLP([hidden_channels, hidden_channels, out_channels], norm=None, dropout=dropout)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        batch_size = batch.max().item() + 1\n",
        "\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = conv(x, edge_index).relu()\n",
        "            x = bn(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        x = global_mean_pool(x, batch, size=batch_size)\n",
        "        return self.mlp(x)\n",
        "\n",
        "model = GIN(dataset.num_features, 16, dataset.num_classes, num_layers=2)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nlzR9qK17lt"
      },
      "source": [
        "### GCN + Node Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuhNJfIm16sZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NodeNorm(nn.Module):\n",
        "    def __init__(self, nn_type=\"n\", unbiased=False, eps=1e-5, power_root=2):\n",
        "        super(NodeNorm, self).__init__()\n",
        "        self.unbiased = unbiased\n",
        "        self.eps = eps\n",
        "        self.nn_type = nn_type\n",
        "        self.power = 1 / power_root\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.nn_type == \"n\":\n",
        "            mean = torch.mean(x, dim=1, keepdim=True)\n",
        "            std = (\n",
        "                torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps\n",
        "            ).sqrt()\n",
        "            x = (x - mean) / std\n",
        "        elif self.nn_type == \"v\":\n",
        "            std = (\n",
        "                torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps\n",
        "            ).sqrt()\n",
        "            x = x / std\n",
        "        elif self.nn_type == \"m\":\n",
        "            mean = torch.mean(x, dim=1, keepdim=True)\n",
        "            x = x - mean\n",
        "        elif self.nn_type == \"srv\":  # square root of variance\n",
        "            std = (\n",
        "                torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps\n",
        "            ).sqrt()\n",
        "            x = x / torch.sqrt(std)\n",
        "        elif self.nn_type == \"pr\":\n",
        "            std = (\n",
        "                torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps\n",
        "            ).sqrt()\n",
        "            x = x / torch.pow(std, self.power)\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        original_str = super().__repr__()\n",
        "        components = list(original_str)\n",
        "        nn_type_str = f\"nn_type={self.nn_type}\"\n",
        "        components.insert(-1, nn_type_str)\n",
        "        new_str = \"\".join(components)\n",
        "        return new_str\n",
        "\n",
        "def get_normalization(norm_type, num_channels=None):\n",
        "    if norm_type is None:\n",
        "        norm = None\n",
        "    elif norm_type == \"batch\":\n",
        "        norm = nn.BatchNorm1d(num_features=num_channels)\n",
        "    elif norm_type == \"node_n\":\n",
        "        norm = NodeNorm(nn_type=\"n\")\n",
        "    elif norm_type == \"node_v\":\n",
        "        norm = NodeNorm(nn_type=\"v\")\n",
        "    elif norm_type == \"node_m\":\n",
        "        norm = NodeNorm(nn_type=\"m\")\n",
        "    elif norm_type == \"node_srv\":\n",
        "        norm = NodeNorm(nn_type=\"srv\")\n",
        "    elif norm_type.find(\"node_pr\") != -1:\n",
        "        power_root = norm_type.split(\"_\")[-1]\n",
        "        power_root = int(power_root)\n",
        "        norm = NodeNorm(nn_type=\"pr\", power_root=power_root)\n",
        "    elif norm_type == \"layer\":\n",
        "        norm = nn.LayerNorm(normalized_shape=num_channels)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o236Y8nj2JFg"
      },
      "outputs": [],
      "source": [
        "from torch.nn import BatchNorm1d, Dropout\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, norm_type=\"node_n\"):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "\n",
        "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
        "        self.bns.append(get_normalization(norm_type=norm_type, num_channels=hidden_dim))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "            self.bns.append(get_normalization(norm_type=norm_type, num_channels=hidden_dim))\n",
        "\n",
        "        self.fc = Linear(hidden_dim, output_dim)\n",
        "        # self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = bn(x)\n",
        "            # x = self.dropout(x)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-3be931R5W3"
      },
      "source": [
        "### Train and test + Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrwRycw4n6Kd"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = F.cross_entropy(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        all_preds.append(out.argmax(dim=1).cpu().numpy())\n",
        "        all_labels.append(data.y.cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
        "    train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    return accuracy, train_loss, f1\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.cross_entropy(out, data.y)\n",
        "            total_loss += loss.item()\n",
        "            all_preds.append(out.argmax(dim=1).cpu().numpy())\n",
        "            all_labels.append(data.y.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
        "    val_loss = total_loss / len(loader)\n",
        "\n",
        "    return accuracy, val_loss, f1, all_preds, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aM6B7qVBR-c4",
        "outputId": "f27e7be5-363f-41dc-e7db-2d9a7c20395f"
      },
      "outputs": [],
      "source": [
        "num_layers_list = [2, 3, 4]\n",
        "hidden_dims_list = [32, 64, 128]\n",
        "norm_types_list = [\"layer\",\"node_n\", \"node_v\", \"node_m\",\"node_srv\",\"node_pr_2\"] #\n",
        "param_grid = list(product(num_layers_list, hidden_dims_list, norm_types_list))\n",
        "\n",
        "best_val_acc = 0\n",
        "best_params = None\n",
        "best_model_path = \"best_grid_model.pt\"\n",
        "results = []\n",
        "\n",
        "\n",
        "for num_layers, hidden_dim, norm_type in param_grid:\n",
        "    print(f\"\\nTraining with num_layers={num_layers}, hidden_dim={hidden_dim}, norm_type={norm_type}\")\n",
        "\n",
        "    set_seed(42)\n",
        "\n",
        "    model = GCN(dataset.num_features, hidden_dim=hidden_dim, output_dim=dataset.num_classes, num_layers=num_layers, norm_type=norm_type)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    patience = 20\n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(100):\n",
        "        train_acc, train_loss, train_f1 = train()\n",
        "        val_acc, val_loss, val_f1, _, _ = evaluate(val_loader)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Train F1: {train_f1:.4f}, \"\n",
        "            f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}, Val F1: {val_f1:.4f}\"\n",
        "        )\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"temp_model.pt\")  # Save the best model for this config\n",
        "            print(f\"Saved best model at epoch {epoch} with Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Train F1: {train_f1:.4f}, \"\n",
        "            f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    # Load best model for this configuration\n",
        "    model.load_state_dict(torch.load(\"temp_model.pt\"))\n",
        "    test_acc, test_loss, test_f1,_,_ = evaluate(test_loader)\n",
        "    results.append({\n",
        "                \"num_layers\": num_layers,\n",
        "                \"hidden_dim\": hidden_dim,\n",
        "                \"norm_type\": norm_type,\n",
        "                \"test_accuracy\": test_acc,\n",
        "                \"test_loss\": test_loss,\n",
        "                \"test_f1\": test_f1\n",
        "            })\n",
        "\n",
        "    print(f\"For num_layers={num_layers}, hidden_dim={hidden_dim}: Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}, Test F1: {test_f1:.4f}\")\n",
        "\n",
        "    # Update best model if current one is better\n",
        "    if test_acc > best_val_acc:\n",
        "        best_val_acc = test_acc\n",
        "        best_params = (num_layers, hidden_dim, norm_type)\n",
        "        torch.save(model.state_dict(), best_model_path)  # Save the best model\n",
        "\n",
        "# Final results\n",
        "print(f\"\\nBest Model: num_layers={best_params[0]}, hidden_dim={best_params[1]},  norm_type={best_params[2]} with Accuracy={best_val_acc:.4f}\")\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results_correct = df_results['test_accuracy'].round(4)*100\n",
        "df_results_correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx-_lS6X_omc"
      },
      "source": [
        "# GNNs with edge attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4uTszKqP7RL"
      },
      "source": [
        "### GCN + Node Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXb70ifIP7RL"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NodeNorm(nn.Module):\n",
        "    def __init__(self, nn_type=\"n\", unbiased=False, eps=1e-5, power_root=2):\n",
        "        super(NodeNorm, self).__init__()\n",
        "        self.unbiased = unbiased\n",
        "        self.eps = eps\n",
        "        self.nn_type = nn_type\n",
        "        self.power = 1 / power_root\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.nn_type == \"n\":\n",
        "            mean = torch.mean(x, dim=1, keepdim=True)\n",
        "            std = (\n",
        "                torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps\n",
        "            ).sqrt()\n",
        "            x = (x - mean) / std\n",
        "        elif self.nn_type == \"v\":\n",
        "            std = (\n",
        "                torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps\n",
        "            ).sqrt()\n",
        "            x = x / std\n",
        "        elif self.nn_type == \"m\":\n",
        "            mean = torch.mean(x, dim=1, keepdim=True)\n",
        "            x = x - mean\n",
        "        elif self.nn_type == \"srv\":  # square root of variance\n",
        "            std = (\n",
        "                torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps\n",
        "            ).sqrt()\n",
        "            x = x / torch.sqrt(std)\n",
        "        elif self.nn_type == \"pr\":\n",
        "            std = (\n",
        "                torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps\n",
        "            ).sqrt()\n",
        "            x = x / torch.pow(std, self.power)\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        original_str = super().__repr__()\n",
        "        components = list(original_str)\n",
        "        nn_type_str = f\"nn_type={self.nn_type}\"\n",
        "        components.insert(-1, nn_type_str)\n",
        "        new_str = \"\".join(components)\n",
        "        return new_str\n",
        "\n",
        "def get_normalization(norm_type, num_channels=None):\n",
        "    if norm_type is None:\n",
        "        norm = None\n",
        "    elif norm_type == \"batch\":\n",
        "        norm = nn.BatchNorm1d(num_features=num_channels)\n",
        "    elif norm_type == \"node_n\":\n",
        "        norm = NodeNorm(nn_type=\"n\")\n",
        "    elif norm_type == \"node_v\":\n",
        "        norm = NodeNorm(nn_type=\"v\")\n",
        "    elif norm_type == \"node_m\":\n",
        "        norm = NodeNorm(nn_type=\"m\")\n",
        "    elif norm_type == \"node_srv\":\n",
        "        norm = NodeNorm(nn_type=\"srv\")\n",
        "    elif norm_type.find(\"node_pr\") != -1:\n",
        "        power_root = norm_type.split(\"_\")[-1]\n",
        "        power_root = int(power_root)\n",
        "        norm = NodeNorm(nn_type=\"pr\", power_root=power_root)\n",
        "    elif norm_type == \"layer\":\n",
        "        norm = nn.LayerNorm(normalized_shape=num_channels)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FR2QTirP7RM"
      },
      "outputs": [],
      "source": [
        "from torch.nn import BatchNorm1d, Dropout\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, norm_type=\"node_n\"):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "\n",
        "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
        "        self.bns.append(get_normalization(norm_type=norm_type, num_channels=hidden_dim))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "            self.bns.append(get_normalization(norm_type=norm_type, num_channels=hidden_dim))\n",
        "\n",
        "        self.fc = Linear(hidden_dim, output_dim)\n",
        "        # self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = F.relu(conv(x, edge_index, edge_attr))\n",
        "            x = bn(x)\n",
        "            # x = self.dropout(x)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcR3CDIcbw1w"
      },
      "source": [
        "### Train and test + Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8V8SfCtbw1x"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        loss = F.cross_entropy(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        all_preds.append(out.argmax(dim=1).cpu().numpy())\n",
        "        all_labels.append(data.y.cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
        "    train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    return accuracy, train_loss, f1\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "            loss = F.cross_entropy(out, data.y)\n",
        "            total_loss += loss.item()\n",
        "            all_preds.append(out.argmax(dim=1).cpu().numpy())\n",
        "            all_labels.append(data.y.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    accuracy = (all_preds == all_labels).mean()\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
        "    val_loss = total_loss / len(loader)\n",
        "\n",
        "    return accuracy, val_loss, f1, all_preds, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6fi0xCoYbw1x",
        "outputId": "d3b86033-944d-410c-8874-16d5a9065afc"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_layers_list = [2, 3, 4]\n",
        "hidden_dims_list = [32, 64, 128]\n",
        "norm_types_list = [\"layer\", \"node_n\", \"node_v\", \"node_m\",\"node_srv\",\"node_pr_2\"] #\n",
        "param_grid = list(product(num_layers_list, hidden_dims_list, norm_types_list))\n",
        "\n",
        "best_val_acc = 0\n",
        "best_params = None\n",
        "best_model_path = \"best_grid_model_1.pt\"\n",
        "results = []\n",
        "\n",
        "\n",
        "for num_layers, hidden_dim, norm_type in param_grid:\n",
        "    print(f\"\\nTraining with num_layers={num_layers}, hidden_dim={hidden_dim}, norm_type={norm_type}\")\n",
        "\n",
        "    set_seed(42)\n",
        "\n",
        "    model = GCN(dataset.num_features, hidden_dim=hidden_dim, output_dim=dataset.num_classes, num_layers=num_layers, norm_type=norm_type)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    patience = 20\n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(100):\n",
        "        train_acc, train_loss, train_f1 = train()\n",
        "        val_acc, val_loss, val_f1, _, _ = evaluate(val_loader)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Train F1: {train_f1:.4f}, \"\n",
        "            f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}, Val F1: {val_f1:.4f}\"\n",
        "        )\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"temp_model_1.pt\")  # Save the best model for this config\n",
        "            print(f\"Saved best model at epoch {epoch} with Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Train F1: {train_f1:.4f}, \"\n",
        "            f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    # Load best model for this configuration\n",
        "    model.load_state_dict(torch.load(\"temp_model_1.pt\"))\n",
        "    test_acc, test_loss, test_f1,_,_ = evaluate(test_loader)\n",
        "\n",
        "    print(f\"For num_layers={num_layers}, hidden_dim={hidden_dim}, norm_type={norm_type}: Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}, Test F1: {test_f1:.4f}\")\n",
        "    results.append({\n",
        "                \"num_layers\": num_layers,\n",
        "                \"hidden_dim\": hidden_dim,\n",
        "                \"norm_type\": norm_type,\n",
        "                \"test_accuracy\": test_acc,\n",
        "                \"test_loss\": test_loss,\n",
        "                \"test_f1\": test_f1\n",
        "            })\n",
        "    # Update best model if current one is better\n",
        "    if test_acc > best_val_acc:\n",
        "        best_val_acc = test_acc\n",
        "        best_params = (num_layers, hidden_dim, norm_type)\n",
        "        torch.save(model.state_dict(), best_model_path)  # Save the best model\n",
        "\n",
        "# Final results\n",
        "print(f\"\\nBest Model: num_layers={best_params[0]}, hidden_dim={best_params[1]},  norm_type={best_params[2]} with Accuracy={best_val_acc:.4f}\")\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results_correct = df_results['test_accuracy'].round(4)*100\n",
        "df_results_correct\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Eg2AC5ttU74H",
        "5YT6tSG84_rR",
        "AipOk_gaf4HL",
        "iwqSoZ-ASgtM",
        "dKfkuV7NX-nw",
        "Maxtam4t0ee5",
        "0SYgl_Cv8NW5",
        "Y5gfEDW7aESm",
        "_9VkdgqMsi0Z",
        "jLRfDt2Ux5fN",
        "T4rCFG8LcmLq",
        "Ek88DZVppIJU",
        "smrEiWukbE7w",
        "h6h1Rri7T_7c",
        "p6y-_J2sNrlE",
        "IP4kI8B_iEEI",
        "Js8xoyewiEEJ",
        "TWitFPKYiEEK",
        "jPiD-dwpiEEK",
        "hTDyohdO9LqJ",
        "YLXts5uJiEEL",
        "O2nF6_kl0lnk",
        "bZZPBSpxa5na",
        "fft9RcsIiEEM",
        "XNHZQM3H3k7f",
        "usqHDtP65tSy",
        "oNbkuagmeAGz",
        "vN1eRQRQeKWv",
        "Bj9_S071AMsJ",
        "ux-c7yNAUNFQ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
